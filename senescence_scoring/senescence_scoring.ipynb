{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.misc\n",
    "from scipy import ndimage\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model, model_from_json\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell cropping from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def file_names_confirmation(directory_path):\n",
    "    \n",
    "    global directory_names, directory_names_list\n",
    "    \n",
    "    directory_names = glob.glob(directory_path + \"*\")\n",
    "    directory_names_list = [os.path.basename(r) for r in directory_names]\n",
    "   \n",
    "    print(directory_names_list)\n",
    "    print(directory_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cell_cropper_list(directory_path):\n",
    "    global directory_names\n",
    "    \n",
    "    directory_names = glob.glob(directory_path + \"*\")\n",
    "    print(directory_names)\n",
    "    for i in directory_names:\n",
    "        files = glob.glob(i + \"/*\")\n",
    "        directory_name = os.path.basename(i)\n",
    "        print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cell_crop_single(single_img, binimg_thred = 5., min_area=50, scale_v=25, scale_h=25, chs=0, fluoro=False):\n",
    "    \n",
    "    cells = np.empty((0, scale_v*2, scale_h*2, 3))\n",
    "    \n",
    "    img = single_img.astype(np.uint8)\n",
    "    img_chs = cv2.split(img)\n",
    "    img_preprocessed = cv2.GaussianBlur(img_chs[chs],(5,5),0)\n",
    "    if fluoro==False:\n",
    "        binimg = (img_preprocessed < np.percentile(img_preprocessed, binimg_thred))\n",
    "        binimg = binimg.astype(np.uint8)\n",
    "    else:\n",
    "        binimg = (img_preprocessed > np.percentile(img_preprocessed, binimg_thred))\n",
    "        binimg = binimg.astype(np.uint8)\n",
    "\n",
    "    img_, contours, _ = cv2.findContours(binimg, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "    arr=[]\n",
    "    \n",
    "    start=np.empty((0,2))\n",
    "    start=np.append(start,np.array([[0, 0]]),axis=0)\n",
    "    \n",
    "    for j in contours:\n",
    "        if cv2.contourArea(j)<min_area:\n",
    "            continue\n",
    "        x_=0\n",
    "        y_=0\n",
    "        for k in j:\n",
    "            x_ += k[0][0]\n",
    "            y_ += k[0][1]\n",
    "        arr.append([x_/len(j), y_/len(j)])\n",
    "    arr = np.array(arr)\n",
    "    \n",
    "    \n",
    "    for j in range(len(arr)):\n",
    "    \n",
    "        if (arr[j][1] < scale_v) or (arr[j][1] > img.shape[0]-scale_v) or (arr[j][0] < scale_h) or (arr[j][0] > img.shape[1]-scale_h):\n",
    "            continue \n",
    "        \n",
    "        top = int(arr[j][1])-scale_v\n",
    "        bottom = int(arr[j][1])+scale_v\n",
    "    \n",
    "        left = int(arr[j][0])-scale_h\n",
    "        right = int(arr[j][0])+scale_h\n",
    "    \n",
    "        if left < 0:\n",
    "            left = 0\n",
    "            right = scale_h*2\n",
    "        if right > img.shape[1]:\n",
    "            right = img.shape[1]\n",
    "            left = img.shape[1]-scale_h*2\n",
    "    \n",
    "        if top < 0:\n",
    "            top = 0\n",
    "            bottom = scale_v*2\n",
    "        if bottom > img.shape[0]:\n",
    "            bottom = img.shape[0]\n",
    "            top = img.shape[0]-scale_v*2      \n",
    "                \n",
    "        img_crop = np.array(img[top:bottom,left:right]).reshape(scale_v*2, scale_h*2, 3).astype(np.uint8)\n",
    "        img_chs = cv2.split(img_crop)\n",
    "        img_preprocessed = cv2.GaussianBlur(img_chs[chs],(5,5),0)\n",
    "            \n",
    "        if fluoro==False:\n",
    "            binimg = (img_preprocessed < np.percentile(img_preprocessed, binimg_thred))\n",
    "            binimg = binimg.astype(np.uint8)\n",
    "        else:\n",
    "            binimg = (img_preprocessed > np.percentile(img_preprocessed, binimg_thred))\n",
    "            binimg = binimg.astype(np.uint8)\n",
    "\n",
    "        img_, contours, _ = cv2.findContours(binimg, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "            \n",
    "        contourArea = []\n",
    "            \n",
    "        for j in contours:\n",
    "            contourArea.append(cv2.contourArea(j))\n",
    "        contourArea_sum = sum(contourArea)\n",
    "        if contourArea_sum<min_area:\n",
    "            continue\n",
    "    \n",
    "        cells = np.append(cells,np.array(img[top:bottom,left:right]).reshape(1,scale_v*2, scale_h*2, 3),axis=0)\n",
    "\n",
    "    print(\"cropped_cell_count:\", cells.shape[0])\n",
    "    \n",
    "    return cells    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cell_crop_from_each_pic(filenames):\n",
    "    \n",
    "    global binimg_thred, min_area, scale_v, scale_h, chs, fluoro\n",
    "    \n",
    "    total_cells = np.empty((0, scale_v*2, scale_h*2, 3))\n",
    "    \n",
    "    for filename in filenames:\n",
    "        print(\"filename:\", filename)\n",
    "        img = scipy.misc.imread(filename)\n",
    "        height, width, chan = img.shape\n",
    "        assert chan == 3\n",
    "        cells = cell_crop_single(img, binimg_thred=binimg_thred, min_area=min_area, scale_v=scale_v, scale_h=scale_h, chs=chs, fluoro=fluoro)\n",
    "        total_cells = np.append(total_cells,cells,axis=0)\n",
    "    \n",
    "    print(\"total_cropped_cell_count:\", total_cells.shape[0])    \n",
    "    \n",
    "    return total_cells  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_cell_cropper(base_directory_name, directory_path, save_directory_npy):\n",
    "    \n",
    "    for i in directory_names:\n",
    "        filenames = glob.glob(i + \"/*\")\n",
    "        directory_name = os.path.basename(i)\n",
    "        cells = cell_crop_from_each_pic(filenames)\n",
    "        save_name = save_directory_npy + base_directory_name + \"_\" + directory_name\n",
    "        np.save(save_name +\".npy\", cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confirmation for cell cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binarization_check(check_directory, binimg_thred = 5., fluoro=False):\n",
    "\n",
    "    check_files = glob.glob(check_directory + \"/*\")    \n",
    "\n",
    "    for i in range(len(check_files)):\n",
    "        picture = scipy.misc.imread(check_files[i])\n",
    "        height, width, chan = picture.shape\n",
    "        assert chan == 3\n",
    "        img = picture.astype(np.uint8)\n",
    "        \n",
    "        print(check_files)\n",
    "        print(\"\")\n",
    "        print(\"##### original picture #####\")\n",
    "        plt.figure(figsize=(10,7.5))\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "\n",
    "        img_chs = cv2.split(img)\n",
    "\n",
    "        img_preprocessed = cv2.GaussianBlur(img_chs[chs],(5,5),0)\n",
    "        if fluoro==False:\n",
    "            binimg = (img_preprocessed < np.percentile(img_preprocessed, binimg_thred))\n",
    "            binimg = binimg.astype(np.uint8)\n",
    "        else:\n",
    "            binimg = (img_preprocessed > np.percentile(img_preprocessed, binimg_thred))\n",
    "            binimg = binimg.astype(np.uint8)\n",
    "    \n",
    "        print(\"\")\n",
    "        print(\"##### post binarization #####\")\n",
    "        plt.figure(figsize=(10,7.5))\n",
    "        plt.imshow(binimg)\n",
    "        plt.colorbar()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cell_crop_check_from_npy(save_directory_npy, npy_file_name):\n",
    "    \n",
    "    test_data_path = sorted(glob.glob(save_directory_npy + npy_file_name))\n",
    "    file_names = [os.path.basename(r) for r in test_data_path]\n",
    "    print(file_names)\n",
    "    \n",
    "    for j in range(len(test_data_path)):\n",
    "        npy = np.load(test_data_path[j])\n",
    "        rand = np.random.randint(0, len(npy), 50)\n",
    "        print(file_names[j])\n",
    "    \n",
    "        for i in range(len(rand)):\n",
    "            img = npy[rand[i]]\n",
    "            rimg = 255 - img\n",
    "            plt.subplot(5,10,i+1, xticks=[], yticks=[])\n",
    "            plt.imshow(rimg)\n",
    "    \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# senescence scoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_validation_list(save_directory_result, save_directory_npy):\n",
    "    \n",
    "    global index_list, test_data_path, save_name_result\n",
    "    \n",
    "    directory_names = glob.glob(directory_path + \"*\")\n",
    "    directory_names_list = [os.path.basename(r) for r in directory_names]\n",
    "    \n",
    "    test_data_directory = save_directory_npy\n",
    "    index_list = sorted(directory_names_list)\n",
    "    print(index_list)\n",
    "\n",
    "    test_data_path = sorted(glob.glob(test_data_directory + npy_file_name))\n",
    "    print(test_data_path)\n",
    "\n",
    "    save_name_result = save_directory_result + base_directory_name\n",
    "    print(save_name_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_input(x0):\n",
    "    return ((x0/255.)-0.5)*2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_test(test_data_path, save_name_result, index_list):\n",
    "    \n",
    "    global classes_ratio, probs_mean, probs\n",
    "    \n",
    "    model = model_from_json(open(json_file_path).read())\n",
    "    model.load_weights(h5_file_path)\n",
    "    \n",
    "    test_data = []\n",
    "    classes = []\n",
    "    classes_ratio = []\n",
    "\n",
    "    for i in range(len(test_data_path)):\n",
    "        data = preprocess_input(np.load(test_data_path[i]))\n",
    "        test_data.append(data)\n",
    "        pred_class =model.predict_classes(data, batch_size=100)\n",
    "        classes.append(pred_class)\n",
    "        classes_ratio.append(sum(pred_class)/len(pred_class))\n",
    "\n",
    "    df_classes_ratio = pd.DataFrame({'mean':classes_ratio}, index=index_list)\n",
    "    df_classes = pd.DataFrame(classes, index=index_list)\n",
    "    classes_data = pd.concat([df_classes_ratio, df_classes], axis=1)\n",
    "    classes_data.to_csv(save_name_result + '_with_class' + '.csv')\n",
    "    df_classes_ratio.to_csv(save_name_result + '_ratio' + '.csv')    \n",
    "  \n",
    "    probs = []\n",
    "    probs_mean = []\n",
    "\n",
    "    for i in range(len(test_data)):\n",
    "        probs.append(model.predict(test_data[i], batch_size=100, verbose=1))\n",
    "        probs_mean.append(np.mean(probs[i], axis=0))\n",
    "    \n",
    "    probs_list = []\n",
    "    probs_list_mean = []\n",
    "    \n",
    "    for i in range(len(test_data)):\n",
    "        probs_list.append(probs[i][:,1])\n",
    "        probs_list_mean.append(np.mean(probs_list[i], axis=0))\n",
    "    \n",
    "    df_probs_list_mean = pd.DataFrame({'mean':probs_list_mean}, index=index_list)\n",
    "    df_probs_list = pd.DataFrame(probs_list, index=index_list)\n",
    "    probs_data_list = pd.concat([df_probs_list_mean, df_probs_list], axis=1)\n",
    "    probs_data_list.to_csv(save_name_result + '_probs_with_class' + '.csv')\n",
    "    df_probs_list_mean.to_csv(save_name_result + '_probes_mean' + '.csv')\n",
    "    \n",
    "    \n",
    "    print('classes_ratio: {0}'.format(classes_ratio))\n",
    "    print('probs_list_mean: {0}'.format(probs_list_mean))\n",
    "    \n",
    "    left = list(range(1, (len(index_list)+1)))\n",
    "    \n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.bar(left, classes_ratio,  tick_label=index_list, align=\"center\")\n",
    "    plt.title(\"classes_ratio\")\n",
    "    plt.xlabel(\"Condition\")\n",
    "    plt.ylabel(\"Classes_ratio\")\n",
    "    plt.grid(True)\n",
    "    print(index_list)\n",
    "    \n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.bar(left, probs_list_mean,  tick_label=index_list, align=\"center\")\n",
    "    plt.title(\"probs_mean\")\n",
    "    plt.xlabel(\"Condition\")\n",
    "    plt.ylabel(\"probs_mean\")\n",
    "    plt.grid(True)\n",
    "    print(index_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# directory and file name\n",
    "\n",
    "base_directory_name = 'test_data'    #Directory name to be tested\n",
    "npy_file_name = 'test_*.npy'    #Numpy file names\n",
    "save_directory_npy = '/home/Demo/npy/scoring/'      #Directory path of numpy files\n",
    "directory_path = \"/home/Demo/pics/scoring/\" + base_directory_name + \"/\"    #Directory path te be tested\n",
    "save_directory_result = '/home/Demo/save_data/scoring/'    # Directory path to save results\n",
    "\n",
    "directory_names = glob.glob(directory_path + \"*\")\n",
    "directory_names_list = [os.path.basename(r) for r in directory_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parameters for cell cropping\n",
    "\n",
    "binimg_thred = 5   # Threshold for image binarization(recomendation value: 1 - 10)\n",
    "n_chan=3               # Number of channels\n",
    "chs = 0                    # Channels for image binarization (0:Red, 1:Green, 2:Blue)\n",
    "fluoro = False         # True: Image binarization for fluorescent images, False: Image binarization for pahse contrast images\n",
    "min_area = 50       # Minimam area for cell detection, to exclude noises)\n",
    "\n",
    "scale_v = 25         # Image width/2 (px)\n",
    "scale_h = 25         # Image height/2 (px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load trained CNN data\n",
    "json_file_path = '/home/Demo/save_data/training_save_data/test.json'\n",
    "h5_file_path = '/home/Demo/save_data/training_save_data/test.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Do senescence scoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Confirmation for file names to be tested\n",
    "file_names_confirmation(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Confirmation for cell cropping files\n",
    "cell_cropper_list(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Do cell cropping\n",
    "do_cell_cropper(base_directory_name, directory_path, save_directory_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Do senescence scoring\n",
    "model_validation_list(save_directory_result, save_directory_npy)\n",
    "\n",
    "print(index_list)\n",
    "model_test(test_data_path, save_name_result, index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
